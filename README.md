# 树注意力机制

README.pdf : 手写说明

tree.py : 代码

大龄闲散失业社会人员，新时代孔乙己，业余时间再创业计划，小丑式科研：

开发一种新的神经网络。

If you are English speaker, this page is talking about a idea called "Tree-based Attention", I am trying to develop a new model to learn non-linear function. You might need a translation tool for more information. 

感兴趣可以与我联系，一起讨论：

- email 510481609@qq.com , zhangjd26@mail2.sysu.edu.cn
- wechat 13016606412

## 观点

- 应该将非线性函数看作一种字典，因为一个非线性函数的输出可以相较于它的输入有任意程度的复杂性。一个非线性函数只能够保证对于同样输入，会给出同样的输出，而无法保证其他任何事情。在一个非线性函数内，它的输入输出的映射规则可以是有规律的，也可以是毫无规律可言的。
- 对于字典这个数据结构而言，二叉树是一种非常优秀的实现方式，使用二叉树进行检索时，只需要付出 O(log n) 的时间开销，就能够从茫茫辞海中找到想要的值，关键是要构建出有效的索引或者说匹配规则。
- 仅仅使用二进制，就可以描述这个世界上许许多多的事情。因此，研究二进制变量的非线性函数，既能对问题有一定程度的简化，也不至于丧失掉一般性。
- 蒙特卡洛树，https://zh.wikipedia.org/wiki/蒙特卡洛树搜索
- 可信、可解释的模型，允许开发者有丰富的手段去分析、改进、限制模型的行为。
- 当使用深度学习进行分类时，如果有n种类别，那么这n种类别都要在前向过程中占据自己的一块内存，这其实非常浪费内存，不利于端侧部署。神经网络的激活实际上非常稀疏，不需要占据这么多的内存

## 信念

如何在模型中刻画一个命题为真的信念，或者为否的信念呢？

我是这样做的，假设命题A的信念为a，且a为一个实数，取值范围是负无穷到正无穷。这意味着模型认为命题A为真的概率为$P(A) = \frac{1}{1+e^{-a}}$，命题A为假的概率为$P(\lnot A) = \frac{1}{1+e^{a}}$。

设有两个命题，命题A和命题B，它们的信念分别是a和b。

若命题C=“命题A和命题B同时发生”，那么命题C的信念$c=-log(e^{-a}+e^{-b})$，

$$P(C)=\frac{1}{1+e^{-a}+e^{-b}} < \min(\frac{1}{1+e^{-a}}, \frac{1}{1+e^{-b}})$$

若命题D=“命题A或命题B发生”，那么命题D的信念$d=log(e^{a}+e^{b})$，

$$P(D)=\frac{1}{1+(e^{a}+e^{b})^{-1}} > \max(\frac{1}{1+e^{-a}}, \frac{1}{1+e^{-b}})$$

若命题F=“命题A为假”，那么命题F的信念$f=-a$，

我考虑在我的神经网络中仅仅包含上面介绍的三种运算，也就是信念的与或非运算。我将去除前向过程中任何的实数乘法、实数加法、激活函数、正则化层。

以后使用如下记号：

$$a \land b = - log(e^{-a} + e^{-b})$$
$$a \lor b = log(e^{a} + e^{b})$$
$$\lnot a = -a$$

## 树

![alt text](<img/tree.png>)

可以将一个二进制非线性函数看作是一个二叉树字典，函数的输入被称为query，树的非叶节点称为key，叶节点称为value。一个非线性函数的映射过程建模为二叉树从根节点到叶节点的搜索过程。

query是一个有限长度的信念的向量，
$$q=\begin{bmatrix}
q_1, q_2,\cdots,q_n
\end{bmatrix}$$

key是两个有限长度的信念的向量，
$$k=\begin{bmatrix}
k_{11}, k_{12},\cdots,k_{1n}\\
k_{21}, k_{22},\cdots,k_{2n}
\end{bmatrix}$$

value是一个有限长度的信念的向量，
$$v=\begin{bmatrix}
v_1, v_2,\cdots,v_m
\end{bmatrix}$$

一个query和一个key的是否匹配的信念是这样计算的

$$
m(q, k) = \cap_i (k_{1i} \land q_i) \lor (k_{2i} \land \lnot q_i)
$$

计算出匹配信念之后，计算sigmod作为概率进行采样（投硬币），如果投到是0，那么走左边，这个选择的信念是 $\lnot m(q, k)$；如果投到是1，走右边，信念$m(q, k)$。

对于二叉树上每一条路径，计算每一个选择的信念，并使用“与”运算来计算这几个选择同时发生的信念。最后模型的输出为

$$
tree(q) = (s, v)
$$

s为树模型对于q来说选择v的信念，由于树模型的前向过程是一个不断投硬币的过程，在模型未收敛之前，模型的输出是根据概率随机的，当模型收敛时，信念会比较高。

## 损失函数

树选择路径的信念值是损失函数重要组成部分，正是它关联起每一次不同的选择。如果希望v符合某一个命题，记为$f(v)$，那么损失函数应该为

$$
loss(s, v) = - logsigmod ((s \land f(v)) \lor \lnot s)
$$

意味着，当v无法符合命题f(v)时，增加$\lnot s$，也就是减少该选择的信念；反之，如果能够满足命题，增加该选择的信念。

树模型就是通过这种类似于蒙特卡洛树的方式，不断投硬币，采样，来学习正确的信念。